{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ed5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e191c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the benefit of setting environmental variable is to protect your api key when sharing code with others\n",
    "# if you don't know how to set env, just copy your api key into the following code \n",
    "# and make sure to remove your key when sharing code with others\n",
    "\n",
    "# openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39290a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_response(sys_msg, prompt, ex, max_tokens):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        #model=\"gpt-3.5-turbo\",\n",
    "        model='gpt-4',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt + \"\\n\" + \"'''\" + ex + \"'''\"\n",
    "            }\n",
    "                ],\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens\n",
    "    )\n",
    "    gpt_answer = response['choices'][0]['message']['content'] \n",
    "    print(\"answer\", gpt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ec7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_responses(sys_msg, prompt, inputs, max_tokens):\n",
    "    for ex in inputs:\n",
    "        get_gpt_response(sys_msg, prompt, ex, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3920b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50\n",
    "sys_message = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa16de",
   "metadata": {},
   "source": [
    "# Category labels for citation purpose classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342948c",
   "metadata": {},
   "source": [
    "Purpose – why an author evaluates the cited article(s);\n",
    "\n",
    "four types of purpose - comparison, critique, use, and info.\n",
    "\n",
    "Sometimes an author cites with an explicit evaluation purpose, such as comparison and critique. Sometimes the evaluation purpose is implicit; for example, if an author cites by saying “we used the method developed by A”, although there seems no evaluation, the author actually endorsed the method by adopting it. Despite such explicit and implicit evaluation, many citations do not show any cues for evaluation purpose. These citations should be annotated as info, indicating the purpose of the citation is to simply point readers to some reference(s) for further reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0a284",
   "metadata": {},
   "source": [
    "# sample test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74dd24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_context</th>\n",
       "      <th>purpose</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For example, some investigators have argued co...</td>\n",
       "      <td>critique</td>\n",
       "      <td>method</td>\n",
       "      <td>4_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several clinical criteria and decision tools h...</td>\n",
       "      <td>critique</td>\n",
       "      <td>method</td>\n",
       "      <td>4_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Importantly the traffic light system missed a ...</td>\n",
       "      <td>critique</td>\n",
       "      <td>method</td>\n",
       "      <td>4_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The NICE fever guidelines advise routine testi...</td>\n",
       "      <td>critique</td>\n",
       "      <td>method</td>\n",
       "      <td>4_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In this context, one can view successful progr...</td>\n",
       "      <td>critique</td>\n",
       "      <td>method</td>\n",
       "      <td>4_positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    citation_context   purpose  aspect  \\\n",
       "0  For example, some investigators have argued co...  critique  method   \n",
       "1  Several clinical criteria and decision tools h...  critique  method   \n",
       "2  Importantly the traffic light system missed a ...  critique  method   \n",
       "3  The NICE fever guidelines advise routine testi...  critique  method   \n",
       "4  In this context, one can view successful progr...  critique  method   \n",
       "\n",
       "    sentiment  \n",
       "0  4_negative  \n",
       "1  4_negative  \n",
       "2  4_negative  \n",
       "3  4_positive  \n",
       "4  4_positive  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "critique_ex = pd.read_excel('data/cora-small-sample-critique.xlsx')\n",
    "critique_ex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c2fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = critique_ex['citation_context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf1728",
   "metadata": {},
   "source": [
    "# intuitive manual prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fff7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Info*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Info*\n",
      "answer *Use*\n",
      "answer *Use*\n"
     ]
    }
   ],
   "source": [
    "# intuitive manual prompt\n",
    "manual_prompt_1 = \"What is the function of the citation in the sentence? \\\n",
    "    *Comparison*, *Critique*, *Use*, or *Info* (the priority of Info is the lowest)? \\\n",
    "    The sentence is:\"\n",
    "\n",
    "get_batch_responses(sys_message, manual_prompt_1, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ca9976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Comparison*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Use*\n",
      "answer *Info*\n",
      "answer *Use*\n",
      "answer *Use*\n"
     ]
    }
   ],
   "source": [
    "manual_prompt_2 = \"Annotate the citation function in the following sentence as either \\\n",
    "    *Comparison*, *Critique*, *Use*, or *Info* (the priority of Info is the lowest): \"\n",
    "\n",
    "get_batch_responses(sys_message, manual_prompt_2, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dba7859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer The citation function in the sentence is *Comparison*.\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Use*\n",
      "answer *Info*\n",
      "answer *Use*\n",
      "answer *Use*\n"
     ]
    }
   ],
   "source": [
    "# adding system message\n",
    "\n",
    "sys_message=\"You are a librarian.\"\n",
    "\n",
    "get_batch_responses(sys_message, manual_prompt_2, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f76ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset system message\n",
    "sys_message = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b2fea",
   "metadata": {},
   "source": [
    "# add more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa6aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n"
     ]
    }
   ],
   "source": [
    "# adding definition of labels\n",
    "\n",
    "context = \"Sometimes an author cites with an explicit evaluation purpose,\\\n",
    "    such as comparison and critique. Sometimes the evaluation purpose is implicit;\\\n",
    "    for example, if an author cites by saying 'we used the method developed by A', \\\n",
    "    although there seems no evaluation, the author actually endorsed the method by adopting it.\\\n",
    "    Despite such explicit and implicit evaluation, many citations do not show any cues for evaluation purpose.\\\n",
    "    These citations should be annotated as *Info*, \\\n",
    "    indicating the purpose of the citation is to \\\n",
    "    simply point readers to some reference(s) for further reading.\"\n",
    "    \n",
    "context_prompt_1 = context + manual_prompt_2\n",
    "\n",
    "get_batch_responses(sys_message, context_prompt_1, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafb756",
   "metadata": {},
   "source": [
    "# add Chain-of-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9b65ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Comparison*\n",
      "answer *Comparison*\n",
      "answer *Use*\n",
      "answer *Critique*\n"
     ]
    }
   ],
   "source": [
    "# adding CoT\n",
    "\n",
    "CoT_prompt = \"Think step by step\"\n",
    "CoT_prompt_2 = context_prompt_1 + CoT_prompt\n",
    "get_batch_responses(sys_message, CoT_prompt_2, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6b0b7",
   "metadata": {},
   "source": [
    "# learn how ChatGPT defines the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3856abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer Critique as the purpose of a citation refers to the use of a citation to critically analyze, evaluate, or interpret the cited work. This could involve discussing the strengths and weaknesses of the work, comparing it to other works on the same topic, or providing a personal perspective or interpretation. The citation serves as a way to give credit to the original source of the ideas or information being critiqued, and also allows readers to locate the original work if they wish to explore it further.\n"
     ]
    }
   ],
   "source": [
    "# ask chatGPT how it defines *Critique*\n",
    "\n",
    "prompt = \"How do you define *Critique* as the purpose of a citation\"\n",
    "\n",
    "get_gpt_response(sys_message, prompt, \"\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ad1afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Comparison*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Comparison*\n",
      "answer *Use*\n",
      "answer *Use*\n"
     ]
    }
   ],
   "source": [
    "# define *Critique* as \"Critique as the purpose of a citation refers to the use of a citation to critically analyze, evaluate, or interpret the cited work. This could involve discussing the strengths and weaknesses of the work, comparing it to other works on the same topic, or providing a personal perspective or interpretation.\"\n",
    "# use this definition as context\n",
    "\n",
    "gpt_def_critique = \"Critique as the purpose of a citation \\\n",
    "    refers to the use of a citation to critically analyze, \\\n",
    "    evaluate, or interpret the cited work. This could involve \\\n",
    "    discussing the strengths and weaknesses of the work, \\\n",
    "    comparing it to other works on the same topic, \\\n",
    "    or providing a personal perspective or interpretation.\"\n",
    "\n",
    "context_prompt_2 = gpt_def_critique + manual_prompt_2\n",
    "\n",
    "get_batch_responses(sys_message, context_prompt_2, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db9dfd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Comparison*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Critique*\n",
      "answer *Critique*\n",
      "answer *Use*\n",
      "answer *Use*\n",
      "answer *Use*\n"
     ]
    }
   ],
   "source": [
    "# remove the part related to *Comparison*\n",
    "gpt_def_critique_2 = \"Critique as the purpose of a citation \\\n",
    "    refers to the use of a citation to critically analyze, \\\n",
    "    evaluate, or interpret the cited work. This could involve \\\n",
    "    discussing the strengths and weaknesses of the work, \\\n",
    "    or providing a personal perspective or interpretation.\"\n",
    "\n",
    "context_prompt_3 = gpt_def_critique_2 + manual_prompt_2\n",
    "\n",
    "get_batch_responses(sys_message, context_prompt_3, inputs, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d134a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
